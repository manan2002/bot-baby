{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MDPXp5-X80r"
   },
   "source": [
    "# Download Twitter followers for a set of users\n",
    "\n",
    "### Notebook Author: Nikhil Utane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vp7x7kWeYABh",
    "outputId": "af1a20c2-2262-47f8-e27f-90076bd7860b",
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pip install GetOldTweets3 if you don't already have the package\n",
    "# !pip install GetOldTweets3\n",
    "\n",
    "# Imports\n",
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import time\n",
    "import tweepy\n",
    "import csv\n",
    "import sys\n",
    "from collections import Counter \n",
    "import requests\n",
    "import http.client, urllib\n",
    "import re\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the security tokens from a keys.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import keys #keep keys in separate file, keys.py\n",
    "\n",
    "consumer_key = keys['consumer_key']\n",
    "consumer_secret = keys['consumer_secret']\n",
    "access_token = keys['access_token']\n",
    "access_token_secret = keys['access_token_secret']\n",
    "pushover_token = keys['pushover_token']\n",
    "pushover_user = keys['pushover_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am using Pushover to notify me if any cell stops running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def pushoverNotify():\n",
    "    conn = http.client.HTTPSConnection(\"api.pushover.net:443\")\n",
    "    conn.request(\"POST\", \"/1/messages.json\",\n",
    "      urllib.parse.urlencode({\n",
    "        \"token\": pushover_token,\n",
    "        \"user\": pushover_user,\n",
    "        \"message\": \"Cell finished execution\",\n",
    "      }), { \"Content-type\": \"application/x-www-form-urlencoded\" })\n",
    "    r=conn.getresponse()\n",
    "    print(r.status, r.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Register the magic so that later any cell that we want to be notified on exception can be done\n",
    "@register_cell_magic('handle')\n",
    "def handle(line, cell):\n",
    "    try:\n",
    "        exec(cell)        \n",
    "    except Exception as e:\n",
    "        pushoverNotify()\n",
    "        raise # if you want the full trace-back in the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get List of Followers. \n",
    "#### We are getting the IDs since the rate limit for that is quite high ~45000 per 15 mins vs ~3000 for usernames\n",
    "#### Then we'll convert ID to username and using GetOldTweets3 to download in bulk going as far back as 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the user configuration here\n",
    "side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_folder = \"../data/\" + side + \"/followers/\"\n",
    "tweets_folder = \"../data/\" + side + \"/tweets/\"\n",
    "handles_file = \"../data/\" + side + \"_handles.txt\"\n",
    "followers_id_file = followers_folder + \"all_followers_id.txt\"\n",
    "followers_id_dedup_file = followers_folder + \"all_followers_id_dedup.txt\"\n",
    "followers_username_file = followers_folder + \"all_followers_username.txt\"\n",
    "fetched_username_files = tweets_folder + \"fetched_list.txt\"\n",
    "GetOldTweets3_bin = \"/home/nikhil/packages/GetOldTweets3/bin/GetOldTweets3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Below source code credit: https://gist.github.com/PandaWhoCodes/46f58fdead71f4c71453d9ed1e21adf8\n",
    "# Credentials\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "def get_and_save_followers(user_name):\n",
    "    \"\"\"\n",
    "    get a list of all followers of a twitter account\n",
    "    :param user_name: twitter username without '@' symbol\n",
    "    :return: list of usernames without '@' symbol\n",
    "    \"\"\"\n",
    "    followers = []\n",
    "    with open(followers_folder + user_name + \"_followers_id.csv\", 'w',encoding=\"utf-8\") as output:\n",
    "        for page in tweepy.Cursor(api.followers_ids, screen_name=user_name, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True).pages():\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "            try:\n",
    "                #followers.extend(page)\n",
    "                for user_id in page:\n",
    "                    output.write('%s\\n' % user_id)\n",
    "            except tweepy.TweepError as e:\n",
    "                print(\"Going to sleep:\", e)\n",
    "                # Sleeping to slow down. Else we hit rate limit often\n",
    "                time.sleep(60)\n",
    "    return followers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the initial list of handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "%%handle\n",
    "\n",
    "with open(handles_file) as f:\n",
    "    handles = [line.rstrip() for line in f]\n",
    "    \n",
    "for handle in handles:\n",
    "    print(\"Getting followers for \" + handle)\n",
    "    followers = get_and_save_followers(handle)    \n",
    "    print(\"Done.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge, de-duplicate and sort the followers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "!echo $followers_folder/*.csv | xargs cat > $followers_id_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# initializing list \n",
    "with open(followers_id_file) as f:\n",
    "    id_list = [line.rstrip() for line in f]\n",
    "\n",
    "# printing original list \n",
    "print(\"Number of ids before dedup: %d\" % len(id_list)) \n",
    "\n",
    "# using Counter.most_common() + list comprehension \n",
    "# sorting and removal of duplicates \n",
    "id_dedup = [key for key, value in Counter(id_list).most_common()] \n",
    "\n",
    "# print result \n",
    "print(\"Number of ids after dedup: %d. Percent reduced: %d\" % len(id_dedup), int((len(id_dedup)*100)/len(id_list))) \n",
    "\n",
    "with open(followers_id_dedup_file, \"w\") as output:\n",
    "    for user_id in id_dedup:\n",
    "        output.write('%s\\n' % user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert IDs to usernames for GetOldTweets3 to fetch in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# We are doing a GET on a twitter link and parsing our the username, fastest way with no rate limiting\n",
    "found = not_found = last_index = 0\n",
    "user_list = []\n",
    "\n",
    "# If you are resuming from somewhere in the middle, then uncomment below lines \n",
    "# and specify the last converted ID\n",
    "#print(\"Reading ID file\");\n",
    "#with open(followers_id_dedup_file) as f:\n",
    "#    id_dedup = [line.rstrip() for line in f]\n",
    "\n",
    "#last_index = id_dedup.index(\"7137234657261056\")\n",
    "#del id_dedup[0:last_index+1]\n",
    "\n",
    "with open(followers_username_file, \"w\") as output:\n",
    "    count = last_index + 1\n",
    "    for user_id in id_dedup:\n",
    "        print(\"[%d] Converting %s\" % (count, user_id) , end=' ');\n",
    "        r = requests.get('https://twitter.com/intent/user?user_id=' + user_id)\n",
    "        user_search=re.search('<title>.*\\(@(.*)\\).*</title>', r.content.decode('utf-8'), re.IGNORECASE)\n",
    "        if user_search:\n",
    "            username = user_search.group(1)            \n",
    "            user_list.append(username)\n",
    "            output.write('%s\\n' % username)\n",
    "            found += 1\n",
    "            print(\"=> %s\" % username);\n",
    "        else:\n",
    "            not_found += 1\n",
    "            print(\"ID %s not found\" % user_id);\n",
    "        count += 1\n",
    "        \n",
    "    print(\"%d usernames found. %d not found.\" % found, not_found)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GetOldTweets3 to download tweets upto Jan 2014 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Function the pulls tweets from a specific username and turns to csv file\n",
    "# Parameters: (list of twitter usernames), (max number of most recent tweets to pull from)\n",
    "def username_tweets_to_csv(username, count):\n",
    "    # Creation of query object\n",
    "    tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
    "                                            .setSince(\"2014-01-01\")\\\n",
    "                                            .setMaxTweets(count)\\\n",
    "                                            .setEmoji(\"unicode\")\n",
    "    try:\n",
    "        # Creation of list that contains all tweets\n",
    "        tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "        # Creating list of chosen tweet data\n",
    "        user_tweets = [[tweet.date, tweet.text] for tweet in tweets]\n",
    "\n",
    "        # Creation of dataframe from tweets list\n",
    "        tweets_df = pd.DataFrame(user_tweets, columns = ['Datetime', 'Text'])\n",
    "\n",
    "        # Converting dataframe to CSV\n",
    "        tweets_df.to_csv(tweets_folder + '/original/{}-{}k-tweets.csv'.format(username, int(count/1000)), sep=',')\n",
    "    except:\n",
    "        print(\"Caught Rate limit Exception. Sleeping...\")\n",
    "        time.sleep(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hide_output": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Reading followers\")\n",
    "with open(followers_username_file) as f:\n",
    "    user_list = [line.rstrip() for line in f] \n",
    "\n",
    "print(\"Reading already fetched usernames\")\n",
    "with open(fetched_username_files) as f:\n",
    "    fetched_list = [line.rstrip() for line in f]\n",
    "\n",
    "with open(fetched_username_files, \"a+\") as output:\n",
    "    count = 0\n",
    "    for username in user_list:    \n",
    "        if username not in fetched_list:\n",
    "            print(\"[%d] Fetching tweets for %s\" % (count, username))\n",
    "            username_tweets_to_csv(username, 0)    \n",
    "            output.write('%s\\n' % username)\n",
    "        else:\n",
    "            print(\"User %s already fetched. Skipping\" % username)\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture all tweets into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path one by one and run the below two cells appropriately to generate a dataframe of tweets\n",
    "#path = '/home/nikhil/packages/GetOldTweets3/bin/tweets/' + side \n",
    "#path = tweets_folder + \"/original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "li = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaders_tweets_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_tweets_df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets download using function call of GetOldTweets3 vs using the binary have different casing for column names\n",
    "# So convert below one to lowercase before merging\n",
    "followers_tweets_df.columns = followers_tweets_df.columns.str.lower()\n",
    "followers_tweets_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [leaders_tweets_df, followers_tweets_df]\n",
    "all_tweets_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=tweets_folder + \"/processed\"\n",
    "all_tweets_df.text.to_csv(path+'/all_tweets.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Text Pre-processing - Cleanup tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: \n",
    "# 1) Remove URL\n",
    "# 2) Keep tweets greater than 30 characters\n",
    "\n",
    "MIN_CHARS = 30\n",
    "path=tweets_folder + \"/processed\"\n",
    "\n",
    "with open(path+\"/all_tweets.txt\") as f:\n",
    "    all_tweets = [line.rstrip() for line in f]    \n",
    "    \n",
    "    with open(path+\"/all_tweets_cleaned.txt\", 'w',encoding=\"utf-8\") as output:\n",
    "        for tweet in all_tweets:            \n",
    "            tweet = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", tweet, flags=re.MULTILINE)\n",
    "            tweet = re.sub(\"([^\\x00-\\x7F])+\",\" \",tweet)\n",
    "            tweet = ' '.join(tweet.split()) \n",
    "            tweet = tweet.replace('&amp;', '&')            \n",
    "            if len(tweet) > MIN_CHARS:\n",
    "                output.write('%s\\n' % tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download tweets using the GetOldTweets3 binary (appears to be doing it faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this to download tweets for all the 'leaders' (the original lef or right handles list)\n",
    "#%%handle\n",
    "\n",
    "since_list=['2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01', '2020-01-01']\n",
    "until_list=['2014-12-31', '2015-12-31', '2016-12-31', '2017-12-31', '2018-12-31', '2019-12-31', '2020-12-31']\n",
    "\n",
    "tweets_bin_folder = tweets_folder + \"original/leaders/\"\n",
    "    \n",
    "with open(handles_file) as f:\n",
    "    handles = [line.rstrip() for line in f]\n",
    "    \n",
    "for handle in handles:\n",
    "    print(\"Getting tweets for \" + handle)\n",
    "    \n",
    "    for since, until in zip(since_list, until_list):\n",
    "        outfile = handle + \"_\" + since.split('-')[0] + \".csv\"\n",
    "        cmd = GetOldTweets3_bin + \" --username \" + handle + \\\n",
    "        \" --since \" + since + \" --until \" + until + \\\n",
    "        \" --maxtweets 0 --emoji unicode --output \" + tweets_bin_folder + outfile \n",
    "        print(cmd)\n",
    "        os.system(cmd)\n",
    "        print(\"%s Done.\" % outfile)\n",
    "        time.sleep(60)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some files are not downloaded properly. Re-download them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-download those files which have size 84 bytes\n",
    "\n",
    "all_files = glob.glob(tweets_bin_folder + \"/*.csv\")\n",
    "\n",
    "for filename in all_files:\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size is 84:\n",
    "        filename = os.path.basename(filename)\n",
    "        #print(\"%s is having size 84\" % filename)\n",
    "        handle=filename.split('_20')[0]\n",
    "        year=filename.split('.csv')[0][-4:]\n",
    "        #print(\"Re-fetching for %s & %s\" % (handle, year))\n",
    "        outfile = handle + \"_\" + year + \".csv\"\n",
    "        cmd = GetOldTweets3_bin + \" --username \" + handle + \\\n",
    "        \" --since \" + year + \"-01-01\" + \" --until \" + year + \"-12-31\" + \\\n",
    "        \" --maxtweets 0 --emoji unicode --output \" + tweets_bin_folder + outfile \n",
    "        print(cmd)\n",
    "        os.system(cmd)\n",
    "        print(\"%s Done.\" % outfile)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Unused Code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Not used for now\n",
    "def save_followers_to_csv(user_name, data):\n",
    "    \"\"\"\n",
    "    saves json data to csv\n",
    "    :param data: data recieved from twitter\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    HEADERS = [\"name\", \"screen_name\", \"description\", \"followers_count\", \"followers_count\",\n",
    "               'friends_count', \"listed_count\", \"favourites_count\", \"created_at\"]\n",
    "    with open(followers_folder + user_name + \"_followers.csv\", 'w',encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(HEADERS)\n",
    "        for profile_data in data:\n",
    "            profile = []\n",
    "            for header in HEADERS:\n",
    "                profile.append(profile_data._json[header])\n",
    "            csv_writer.writerow(profile)\n",
    "\n",
    "\n",
    "def save_followers_id_to_file(user_name, data):    \n",
    "    with open(followers_folder + user_name + \"_followers_id.csv\", 'w',encoding=\"utf-8\") as output:\n",
    "        for user_id in data:\n",
    "            output.write('%s\\n' % user_id)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "secrets = {}\n",
    "with open(\"/home/nikhil/my_projects/twitter_keys\") as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    for line in lines:\n",
    "        key, value = line.partition(\"=\")[::2]\n",
    "        secrets[key.strip()] = value"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GetOldTweets3 Twitter Scraper",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "twenv",
   "language": "python",
   "name": "twenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
